{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3cefe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfinnfrei\u001b[0m (\u001b[33mfinnfreiheit\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/finnfreiheit/code/hpi/ahocv/Lab_2/notebooks/wandb/run-20251202_095805-497t7pt5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/finnfreiheit/test_1/runs/497t7pt5' target=\"_blank\">olive-brook-2</a></strong> to <a href='https://wandb.ai/finnfreiheit/test_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/finnfreiheit/test_1' target=\"_blank\">https://wandb.ai/finnfreiheit/test_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/finnfreiheit/test_1/runs/497t7pt5' target=\"_blank\">https://wandb.ai/finnfreiheit/test_1/runs/497t7pt5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▄▅▅█▇██</td></tr><tr><td>loss</td><td>█▅▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.77519</td></tr><tr><td>loss</td><td>0.23788</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">olive-brook-2</strong> at: <a href='https://wandb.ai/finnfreiheit/test_1/runs/497t7pt5' target=\"_blank\">https://wandb.ai/finnfreiheit/test_1/runs/497t7pt5</a><br> View project at: <a href='https://wandb.ai/finnfreiheit/test_1' target=\"_blank\">https://wandb.ai/finnfreiheit/test_1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251202_095805-497t7pt5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import wandb\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"finnfreiheit\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"test_1\",\n",
    "    # Track hyperparameters and run metadata.\n",
    "    config={\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"dataset\": \"CIFAR-100\",\n",
    "        \"epochs\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Simulate training.\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2**-epoch - random.random() / epoch - offset\n",
    "    loss = 2**-epoch + random.random() / epoch + offset\n",
    "\n",
    "    # Log metrics to wandb.\n",
    "    run.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# Finish the run and upload any remaining data.\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23d8ffab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#from assessment import assesment_utils\n",
    "#from assessment.assesment_utils import Classifier\n",
    "#import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77812b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (embedder): Sequential(\n",
       "    (0): Conv2d(1, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=3200, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.models import Classifier\n",
    "\n",
    "lidar_cnn = Classifier(1).to(device)\n",
    "for param in lidar_cnn.parameters():\n",
    "    lidar_cnn.requires_grad = False\n",
    "lidar_cnn.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36a6f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import MyDataset\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VALID_BATCHES = 10\n",
    "N = 9999\n",
    "\n",
    "valid_N = VALID_BATCHES*BATCH_SIZE\n",
    "train_N = N - valid_N\n",
    "\n",
    "train_data = MyDataset(\"../data/assessment/\", 0, train_N)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "valid_data = MyDataset(\"../data/assessment/\", train_N, N)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d56816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import Embedder\n",
    "\n",
    "img_embedder = Embedder(4).to(device)\n",
    "lidar_embedder = Embedder(1).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0388c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ContrastivePretraining\n",
    "\n",
    "CILP_model = ContrastivePretraining(img_embedder=img_embedder, lidar_embedder=lidar_embedder).to(device)\n",
    "optimizer = Adam(CILP_model.parameters(), lr=0.0001)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_lidar = nn.CrossEntropyLoss()\n",
    "ground_truth = torch.arange(BATCH_SIZE, dtype=torch.long).to(device)\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1a03d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CILP_loss(batch):\n",
    "    rbg_img, lidar_depth, class_idx = batch\n",
    "    logits_per_img, logits_per_lidar = CILP_model(rbg_img, lidar_depth)\n",
    "    total_loss = (loss_img(logits_per_img, ground_truth) + loss_lidar(logits_per_lidar, ground_truth))/2\n",
    "    return total_loss, logits_per_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72ea754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train Loss: 3.085106645255144 \n",
      "Similarity:\n",
      "tensor([[0.9960, 0.2664, 0.7968,  ..., 0.5645, 0.4156, 0.3492],\n",
      "        [0.2992, 0.9953, 0.2646,  ..., 0.2267, 0.9766, 0.2520],\n",
      "        [0.7784, 0.3110, 0.9908,  ..., 0.6760, 0.2901, 0.2194],\n",
      "        ...,\n",
      "        [0.5929, 0.2448, 0.6733,  ..., 0.9945, 0.2106, 0.6167],\n",
      "        [0.3771, 0.9698, 0.2555,  ..., 0.2325, 0.9939, 0.2596],\n",
      "        [0.2953, 0.2855, 0.2259,  ..., 0.5776, 0.2670, 0.9963]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.1927100106289514 \n",
      "Similarity:\n",
      "tensor([[0.9885, 0.8773, 0.4653,  ..., 0.5471, 0.5718, 0.9677],\n",
      "        [0.9365, 0.9974, 0.3049,  ..., 0.2914, 0.3206, 0.9615],\n",
      "        [0.4030, 0.2808, 0.9944,  ..., 0.5814, 0.5240, 0.3419],\n",
      "        ...,\n",
      "        [0.4802, 0.2921, 0.5523,  ..., 0.9991, 0.9933, 0.4463],\n",
      "        [0.5170, 0.3323, 0.4893,  ..., 0.9937, 0.9984, 0.4879],\n",
      "        [0.9944, 0.9426, 0.3629,  ..., 0.4923, 0.5222, 0.9962]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Epoch 1\n",
      "Train Loss: 3.0299126904204514 \n",
      "Similarity:\n",
      "tensor([[0.9936, 0.3238, 0.3280,  ..., 0.9738, 0.9026, 0.9368],\n",
      "        [0.3032, 0.9948, 0.9548,  ..., 0.3524, 0.5620, 0.4370],\n",
      "        [0.3323, 0.9846, 0.9942,  ..., 0.3741, 0.6326, 0.4518],\n",
      "        ...,\n",
      "        [0.9932, 0.3867, 0.3838,  ..., 0.9846, 0.9326, 0.9615],\n",
      "        [0.8969, 0.6165, 0.6339,  ..., 0.9043, 0.9967, 0.9164],\n",
      "        [0.9390, 0.4962, 0.4350,  ..., 0.9745, 0.8750, 0.9934]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.18606478289554 \n",
      "Similarity:\n",
      "tensor([[0.9932, 0.8506, 0.4813,  ..., 0.4211, 0.4836, 0.9466],\n",
      "        [0.8859, 0.9981, 0.3603,  ..., 0.2172, 0.2726, 0.9389],\n",
      "        [0.4279, 0.3550, 0.9942,  ..., 0.4853, 0.3596, 0.3315],\n",
      "        ...,\n",
      "        [0.4167, 0.2192, 0.4551,  ..., 0.9970, 0.9783, 0.3840],\n",
      "        [0.4625, 0.2542, 0.3653,  ..., 0.9821, 0.9983, 0.4353],\n",
      "        [0.9764, 0.9222, 0.3281,  ..., 0.3952, 0.4792, 0.9970]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      "Train Loss: 3.02341382894943 \n",
      "Similarity:\n",
      "tensor([[0.9966, 0.6438, 0.4108,  ..., 0.9550, 0.6556, 0.8858],\n",
      "        [0.6126, 0.9944, 0.2230,  ..., 0.5551, 0.2737, 0.4352],\n",
      "        [0.4043, 0.2191, 0.9977,  ..., 0.3276, 0.8185, 0.6952],\n",
      "        ...,\n",
      "        [0.9529, 0.5808, 0.2988,  ..., 0.9952, 0.6012, 0.7914],\n",
      "        [0.6400, 0.2940, 0.8137,  ..., 0.6129, 0.9980, 0.9156],\n",
      "        [0.8478, 0.4409, 0.6979,  ..., 0.7929, 0.9344, 0.9967]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.17956196634393 \n",
      "Similarity:\n",
      "tensor([[0.9908, 0.6225, 0.3609,  ..., 0.5237, 0.6138, 0.9237],\n",
      "        [0.5965, 0.9975, 0.4142,  ..., 0.2548, 0.2670, 0.7686],\n",
      "        [0.3553, 0.4238, 0.9981,  ..., 0.5083, 0.3619, 0.2579],\n",
      "        ...,\n",
      "        [0.6178, 0.2420, 0.4860,  ..., 0.9979, 0.9781, 0.5478],\n",
      "        [0.6965, 0.2488, 0.3584,  ..., 0.9692, 0.9986, 0.6260],\n",
      "        [0.9222, 0.7508, 0.2401,  ..., 0.5132, 0.6097, 0.9982]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from src.visualization import print_CILP_results\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    CILP_model.train()\n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits_per_img = get_CILP_loss(batch)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print_CILP_results(epoch, train_loss/step, logits_per_img, is_train=True)\n",
    "\n",
    "    CILP_model.eval()\n",
    "    valid_loss = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        loss, logits_per_img = get_CILP_loss(batch)\n",
    "        valid_loss += loss.item()\n",
    "    print_CILP_results(epoch, valid_loss/step, logits_per_img, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ebca7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in CILP_model.parameters():\n",
    "    CILP_model.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a1a6ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = nn.Sequential(\n",
    "    nn.Linear(200, 1000),   # input: 200-dim CLIP embedding\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 200 * 4 * 4)       # output: 3200-dim get_embs space\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "475d0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projector_loss(model, batch):\n",
    "    rbg_img, lidar_depth, class_idx = batch\n",
    "    imb_embs = CILP_model.img_embedder(rbg_img)\n",
    "    lidar_emb = lidar_cnn.get_embs(lidar_depth)\n",
    "    pred_lidar_embs = model(imb_embs)\n",
    "    return nn.MSELoss(pred_lidar_embs, lidar_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4a8bf17",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_projector_loss() missing 1 required positional argument: 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m epochs = \u001b[32m40\u001b[39m\n\u001b[32m      4\u001b[39m optimizer = torch.optim.Adam(projector.parameters())\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprojector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_projector_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hpi/ahocv/Lab_2/notebooks/../src/training.py:19\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, optimizer, input_fn, epochs, train_dataloader, valid_dataloader, target_idx)\u001b[39m\n\u001b[32m     17\u001b[39m optimizer.zero_grad()\n\u001b[32m     18\u001b[39m target = batch[target_idx].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m outputs = model(*\u001b[43minput_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     21\u001b[39m loss = loss_func(outputs, target)\n\u001b[32m     22\u001b[39m loss.backward()\n",
      "\u001b[31mTypeError\u001b[39m: get_projector_loss() missing 1 required positional argument: 'batch'"
     ]
    }
   ],
   "source": [
    "from src.training import train_model\n",
    "\n",
    "epochs = 40\n",
    "optimizer = torch.optim.Adam(projector.parameters())\n",
    "train_model(projector, optimizer, get_projector_loss, epochs, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d86c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import RGB2LiDARClassifier\n",
    "\n",
    "my_classifier = RGB2LiDARClassifier(projector=projector, CILP_model=CILP_model, lidar_cnn=lidar_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4f48f3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m         correct += batch_correct\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValid Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect/valid_N\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mget_valid_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mget_valid_metrics\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_valid_metrics\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43mmy_classifier\u001b[49m.eval()\n\u001b[32m      9\u001b[39m     correct = \u001b[32m0\u001b[39m\n\u001b[32m     10\u001b[39m     batch_correct = \u001b[32m0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'my_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "def get_correct(output, y):\n",
    "    zero_tensor = torch.tensor([0]).to(device)\n",
    "    pred = torch.gt(output, zero_tensor)\n",
    "    correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "    return correct\n",
    "\n",
    "def get_valid_metrics():\n",
    "    my_classifier.eval()\n",
    "    correct = 0\n",
    "    batch_correct = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        rbg_img, _, class_idx = batch\n",
    "        output = my_classifier(rbg_img)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, class_idx)\n",
    "        batch_correct = get_correct(output, class_idx)\n",
    "        correct += batch_correct\n",
    "    print(f\"Valid Loss: {loss.item():2.4f} | Accuracy {correct/valid_N:2.4f}\")\n",
    "\n",
    "get_valid_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b76a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "optimizer = torch.optim.Adam(my_classifier.parameters())\n",
    "\n",
    "my_classifier.train()\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    batch_correct = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        rbg_img, _, class_idx = batch\n",
    "        output = my_classifier(rbg_img)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, class_idx)\n",
    "        batch_correct = get_correct(output, class_idx)\n",
    "        correct += batch_correct\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Train Loss: {loss.item():2.4f} | Accuracy {correct/train_N:2.4f}\")\n",
    "    get_valid_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_lab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
